\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{knill1996perception}
\citation{doya2007bayesian}
\citation{pouget2013probabilistic}
\citation{beck2011marginalization}
\citation{ott2006neurodynamics}
\citation{steimer2009belief}
\citation{litvak2009cortical}
\citation{george2009towards}
\citation{grabska2013demixing}
\citation{ma2006bayesian}
\citation{pearl1988probabilistic}
\citation{yedidia2003understanding}
\citation{lee2003hierarchical}
\citation{rao2004hierarchical}
\citation{george2009towards}
\citation{ott2006neurodynamics}
\citation{litvak2009cortical}
\citation{steimer2009belief}
\citation{wainwright2003tree}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{ma2006bayesian}
\citation{jazayeri2006optimal}
\citation{beck2008probabilistic}
\citation{beck2011marginalization}
\citation{graf2011decoding}
\citation{beck2011marginalization}
\citation{beck2011marginalization}
\citation{heeger1992normalization}
\citation{carandini2012normalization}
\citation{rubin2015stabilized}
\citation{vasudeva2015marginalization}
\@writefile{toc}{\contentsline {section}{\numberline {2}Probabilistic Population Codes}{2}{section.2}}
\newlabel{PPC}{{2}{2}{Probabilistic Population Codes}{section.2}{}}
\citation{pearl1988probabilistic}
\citation{yedidia2003understanding}
\citation{lee2003hierarchical}
\citation{rao2004hierarchical}
\citation{ott2006neurodynamics}
\citation{george2009towards}
\citation{litvak2009cortical}
\citation{steimer2009belief}
\citation{yedidia2003understanding}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Key properties of linear PPCs. ({\bf  A}) Two single trial population responses for a particular stimulus, with low and high amplitudes (blue and red). The two projections $\textbf  {a}\cdot {\textbf  {r}}$ and $\textbf  {b}\cdot {\textbf  {r}}$ encode the natural parameters of the posterior. ({\bf  B}) Corresponding posteriors over stimulus variables determined by the responses in panel A. The gain or overall amplitude of the population code is inversely proportional to the variance of the posterior distribution.\relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:PPCFig}{{1}{3}{Key properties of linear PPCs. ({\bf A}) Two single trial population responses for a particular stimulus, with low and high amplitudes (blue and red). The two projections $\textbf {a}\cdot \br $ and $\textbf {b}\cdot \br $ encode the natural parameters of the posterior. ({\bf B}) Corresponding posteriors over stimulus variables determined by the responses in panel A. The gain or overall amplitude of the population code is inversely proportional to the variance of the posterior distribution.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Inference by Tree-based Reparameterization}{3}{section.3}}
\newlabel{TRP}{{3}{3}{Inference by Tree-based Reparameterization}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Graphical Models}{3}{subsection.3.1}}
\newlabel{GMs}{{3.1}{3}{Graphical Models}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Belief Propagation and its neural plausibility}{3}{subsection.3.2}}
\newlabel{BP}{{3.2}{3}{Belief Propagation and its neural plausibility}{subsection.3.2}{}}
\newlabel{belief}{{2}{3}{Belief Propagation and its neural plausibility}{equation.3.2}{}}
\citation{ott2006neurodynamics}
\citation{litvak2009cortical}
\citation{steimer2009belief}
\citation{rao2004hierarchical}
\citation{wainwright2003tree}
\citation{wainwright2003tree}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Tree-based Reparameterization}{4}{subsection.3.3}}
\newlabel{TRPEqn}{{3}{4}{Tree-based Reparameterization}{equation.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visualization of tree reparameterization. ({\bf  A}) A probability distribution is specified by factors $\{\psi _s, \psi _{st}\}$ on a tree graph. ({\bf  B}) An alternative parameterization of the same distribution in terms of the marginals $\{T_s, T_{st}\}$. ({\bf  C}) Two TRP updates for a $3\times 3$ nearest-neighbor grid of variables.\relax }}{4}{figure.caption.3}}
\newlabel{fig:TRP2}{{2}{4}{Visualization of tree reparameterization. ({\bf A}) A probability distribution is specified by factors $\{\psi _s, \psi _{st}\}$ on a tree graph. ({\bf B}) An alternative parameterization of the same distribution in terms of the marginals $\{T_s, T_{st}\}$. ({\bf C}) Two TRP updates for a $3\times 3$ nearest-neighbor grid of variables.\relax }{figure.caption.3}{}}
\newlabel{BPTRP}{{5}{4}{Tree-based Reparameterization}{equation.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Neural implementation of TRP updates}{5}{section.4}}
\newlabel{sec:NeuralTRP}{{4}{5}{Neural implementation of TRP updates}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Updating natural parameters}{5}{subsection.4.1}}
\newlabel{ParamUpdates}{{6}{5}{Updating natural parameters}{equation.4.6}{}}
\newlabel{BiGauss}{{7}{5}{Updating natural parameters}{equation.4.7}{}}
\newlabel{LinearComps}{{8}{5}{Updating natural parameters}{equation.4.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Separation of Time Scales for TRP Updates}{5}{subsection.4.2}}
\citation{beck2011marginalization}
\citation{hayden2010neurons}
\citation{rigotti2013importance}
\citation{raposo2014category}
\newlabel{ContinuousParamUpdates}{{10}{6}{Separation of Time Scales for TRP Updates}{equation.4.10}{}}
\newlabel{DSEq}{{11}{6}{Separation of Time Scales for TRP Updates}{equation.4.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Network Architecture}{6}{subsection.4.3}}
\newlabel{NNet}{{4.3}{6}{Network Architecture}{subsection.4.3}{}}
\newlabel{eq:projection}{{12}{6}{Network Architecture}{equation.4.12}{}}
\newlabel{eq:NeuralTRP}{{13}{6}{Network Architecture}{equation.4.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Distributed, nonlinear, recurrent network of neurons that performs probabilistic inference on a graphical model. ({\bf  A}) This simple case uses distinct subpopulations of neurons to represent different factors in the example model in Figure \ref  {fig:TRP2}A. ({\bf  B}) A cartoon shows how the same distribution can be represented as distinct projections of the distributed neural activity, instead of as distinct populations. In both cases, since the neural activities encode log-probabilities, linear connections are responsible for integrating evidence while nonlinear connections perform marginalization.\relax }}{6}{figure.caption.4}}
\newlabel{NN}{{3}{6}{Distributed, nonlinear, recurrent network of neurons that performs probabilistic inference on a graphical model. ({\bf A}) This simple case uses distinct subpopulations of neurons to represent different factors in the example model in Figure \ref {fig:TRP2}A. ({\bf B}) A cartoon shows how the same distribution can be represented as distinct projections of the distributed neural activity, instead of as distinct populations. In both cases, since the neural activities encode log-probabilities, linear connections are responsible for integrating evidence while nonlinear connections perform marginalization.\relax }{figure.caption.4}{}}
\citation{hayden2010neurons}
\citation{rigotti2013importance}
\citation{raposo2014category}
\citation{savin2014spatio}
\citation{archer2015}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{7}{section.5}}
\newlabel{Experiments}{{5}{7}{Experiments}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Dynamics of neural population activity ({\textbf  {A}}) and the expectation parameters of the posterior distribution that the population encodes ({\textbf  {B}}) for one trial of the tree model in Figure \ref  {fig:TRP2}A. ({\textbf  {C}}) Multiple simulations show that relative error decreases as a function of the ratio of fast and slow timescales $\gamma $.\relax }}{7}{figure.caption.5}}
\newlabel{fig:dynamics}{{4}{7}{Dynamics of neural population activity (\bA ) and the expectation parameters of the posterior distribution that the population encodes (\bB ) for one trial of the tree model in Figure \ref {fig:TRP2}A. (\bC ) Multiple simulations show that relative error decreases as a function of the ratio of fast and slow timescales $\gamma $.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Inference performance of our neural network (blue) and standard loopy belief propagation (red) for a variety of graph topologies: chains, single loops, square grids up to $20\times 20$ and densely connected graphs with up to 25 variables. The expectation parameters (means, covariances) of the pseudomarginals closely match the corresponding parameters for the true marginals.\relax }}{7}{figure.caption.6}}
\newlabel{fig:performance}{{5}{7}{Inference performance of our neural network (blue) and standard loopy belief propagation (red) for a variety of graph topologies: chains, single loops, square grids up to $20\times 20$ and densely connected graphs with up to 25 variables. The expectation parameters (means, covariances) of the pseudomarginals closely match the corresponding parameters for the true marginals.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{7}{section.6}}
\newlabel{Conclusion}{{6}{7}{Conclusion}{section.6}{}}
\citation{vasudeva2015marginalization}
\bibstyle{plos.bst}
\bibdata{InferencePPCs}
\bibcite{knill1996perception}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Network performance is robust to noise, and improves with more neurons. ({\bf  A}) Neural activity performing inference on a $5\times 5$ square grid, in the presence of independent spatiotemporal Gaussian noise of standard deviation 0.1 times the standard deviation of each signal. ({\bf  B}) Expectation parameters (means, variances) of the node pseudomarginals closely match the corresponding parameters for the true marginals, despite the noise. Results are shown for one or five neurons per parameter in the graphical model, and for no noise (i.e. infinitely many neurons).\relax }}{8}{figure.caption.7}}
\newlabel{fig:noiseperformance}{{6}{8}{Network performance is robust to noise, and improves with more neurons. ({\bf A}) Neural activity performing inference on a $5\times 5$ square grid, in the presence of independent spatiotemporal Gaussian noise of standard deviation 0.1 times the standard deviation of each signal. ({\bf B}) Expectation parameters (means, variances) of the node pseudomarginals closely match the corresponding parameters for the true marginals, despite the noise. Results are shown for one or five neurons per parameter in the graphical model, and for no noise (i.e. infinitely many neurons).\relax }{figure.caption.7}{}}
\bibcite{doya2007bayesian}{2}
\bibcite{pouget2013probabilistic}{3}
\bibcite{beck2011marginalization}{4}
\bibcite{ott2006neurodynamics}{5}
\bibcite{steimer2009belief}{6}
\bibcite{litvak2009cortical}{7}
\bibcite{george2009towards}{8}
\bibcite{grabska2013demixing}{9}
\bibcite{ma2006bayesian}{10}
\bibcite{pearl1988probabilistic}{11}
\bibcite{yedidia2003understanding}{12}
\bibcite{lee2003hierarchical}{13}
\bibcite{rao2004hierarchical}{14}
\bibcite{wainwright2003tree}{15}
\bibcite{jazayeri2006optimal}{16}
\bibcite{beck2008probabilistic}{17}
\bibcite{graf2011decoding}{18}
\bibcite{heeger1992normalization}{19}
\bibcite{carandini2012normalization}{20}
\bibcite{rubin2015stabilized}{21}
\bibcite{vasudeva2015marginalization}{22}
\bibcite{hayden2010neurons}{23}
\bibcite{rigotti2013importance}{24}
\bibcite{raposo2014category}{25}
\bibcite{savin2014spatio}{26}
\bibcite{archer2015}{27}
\newlabel{sec:references}{{6}{9}{References}{section*.10}{}}
